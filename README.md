# Monet-to-Photo Style Transfer with CycleGAN ğŸ¨ğŸ“¸

This project implements a CycleGAN model to perform style transfer between Monet paintings and real photographs. The model learns to transform photos into the style of Monet paintings and vice versa, using unpaired image data. It leverages cycle-consistency loss, adversarial loss, and perceptual loss to ensure high-quality image translations. ğŸš€

## Table of Contents ğŸ“‹
- [Overview](#overview) ğŸŒŸ
- [Features](#features) âœ¨
- [Requirements](#requirements) ğŸ› ï¸
- [Dataset](#dataset) ğŸ–¼ï¸
- [Installation](#installation) ğŸ”§
- [Usage](#usage) â–¶ï¸
- [Model Architecture](#model-architecture) ğŸ—ï¸
- [Training](#training) ğŸ“ˆ
- [Evaluation](#evaluation) ğŸ“Š
- [Results](#results) ğŸ¥³
- [Contributing](#contributing) ğŸ¤
- [License](#license) ğŸ“œ

## Overview ğŸŒŸ
CycleGAN is a powerful generative adversarial network that enables image-to-image translation without paired examples. This project uses a U-Net-based generator with ResNet blocks and a PatchGAN discriminator to achieve stunning style transfer between Monet paintings and photographs. The model is trained with adversarial, cycle-consistency, and perceptual losses to produce realistic and coherent outputs. ğŸ¨

## Features âœ¨
- **CycleGAN Architecture**: Two generators (photo-to-Monet and Monet-to-photo) and two discriminators for robust translation. ğŸ”„
- **Perceptual Loss**: Utilizes VGG16 features to enhance the visual quality of generated images. ğŸ–Œï¸
- **Image Buffer**: Prevents mode collapse by storing previously generated images. ğŸ—„ï¸
- **Learning Rate Scheduling**: Dynamically adjusts learning rates during training for better convergence. â³
- **Evaluation Metrics**: Computes FID and MiFID to assess the quality of generated images. ğŸ“

## Requirements ğŸ› ï¸
- Python 3.7+ ğŸ
- PyTorch ğŸ”¥
- torchvision ğŸ“·
- NumPy ğŸ”¢
- Pillow ğŸ–¼ï¸
- SciPy ğŸ”¬
- pandas ğŸ¼
- tqdm â³

## Install the dependencies using:
```bash
pip install torch torchvision numpy pillow scipy pandas tqdm
```


## Dataset ğŸ–¼ï¸
The dataset should include two directories:

photo/: Real photographs in .jpg format. ğŸ“¸
monet/: Monet paintings in .jpg format. ğŸ¨
eval/: Real Monet images for evaluation. ğŸ–Œï¸
Ensure all images are 256x256 pixels. You can use datasets like the Monet2Photo dataset from Kaggle. ğŸ“‚

## Installation ğŸ”§
Clone the repository:

```bash
git clone https://github.com/your-username/Monet-to-Photo-Style-Transfer-with-CycleGAN.git
cd Monet-to-Photo-Style-Transfer-with-CycleGAN

```
## Usage â–¶ï¸
Prepare the Dataset:
Place photo images in the photo/ directory. ğŸ“·
Place Monet paintings in the monet/ directory. ğŸ–¼ï¸
Place evaluation images in the eval/ directory. ğŸ–Œï¸
Train the Model: Run the training script to train the CycleGAN model:
```bash
python main.py
```
This will:
### Train the model for 200 epochs. â³
Save model checkpoints every 20 epochs in the models/ directory. ğŸ’¾
Save final generator models (G.pth and F.pth) in the Final_models/ directory. ğŸ
Generate Monet-style images in the submission/ directory. ğŸ–¼ï¸

### Evaluate the Model: Evaluate the generated images using FID and MiFID metrics:
```bash
python evaluation.py
```
The evaluation script will:
Load real Monet images from eval/ and generated images from submission/. ğŸ“‚
Compute FID and MiFID scores. ğŸ“Š
Save results to submission.csv. ğŸ“„


## Model Architecture ğŸ—ï¸
**Generator**: A U-Net with 9 ResNet blocks, including convolutional layers, instance normalization, and ReLU activations. ğŸ§ 
**Discriminator**: A PatchGAN with multiple convolutional layers and LeakyReLU activations. âš–ï¸
**Loss Functions**:
Adversarial Loss (MSE): Ensures generated images are indistinguishable from real images. ğŸ¯
Cycle-Consistency Loss (L1): Ensures images can be reconstructed after translation. ğŸ”„
Perceptual Loss (L1): Uses VGG16 features to improve visual quality. ğŸ–Œï¸

## Training ğŸ“ˆ
Hyperparameters:
Image Size: 256x256 ğŸ–¼ï¸
Batch Size: 3 ğŸ“¦
Learning Rate: 0.0002 ğŸš€
Beta1: 0.5 âš–ï¸
Epochs: 200 â³
Cycle Loss Weight: 10.0 ğŸ”„
Perceptual Loss Weight: 5.0 ğŸ–Œï¸
Training Process (in main.py):
Alternates between training generators and discriminators. ğŸ”„
Uses Adam optimizers with learning rate scheduling (decays every 100 epochs). ğŸ“‰
Saves checkpoints every 20 epochs and final models at the end. ğŸ’¾

Evaluation ğŸ“Š
Metrics (in evaluation.py):
FID: Measures similarity between feature distributions of real and generated images using InceptionV3. ğŸ“
MiFID: Measures memorization by computing the minimum cosine distance between generated and real image features. ğŸ”
Process:
Subsampling ensures equal numbers of real and generated images. âš–ï¸
Processes images in batches (default batch size: 64). ğŸ“¦
Saves results to submission.csv. ğŸ“„

## Performance Metrics


| Metric       | Value   | Training Details          |
|--------------|---------|---------------------------|
| FID Score    | 46.22   |                           |
| MiFID Score  | 0.222   | Trained for 200 epochs    |
|              |         | Image size: 256Ã—256 px    |

## ğŸ“Œ License

This project is for educational and research use only.
